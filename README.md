# Towards the secrets of sota training strategies
## Welcome
This repoistory is used to organize training strategies 
- Regularization
  - data augmentation | 2017- <a href="https://arxiv.org/abs/1712.04621">Luis, DL based</a> 2018- <a href="https://openaccess.thecvf.com/content_CVPR_2019/papers/Cubuk_AutoAugment_Learning_Augmentation_Strategies_From_Data_CVPR_2019_paper.pdf"> Ekin, AutoAugment(RL Learned)</a> 2019- <a href="https://arxiv.org/abs/1909.13719">Ekin, RandAugment</a>
  - dropout
  - Mixup | 2017 - <a href="https://arxiv.org/abs/1710.09412"> Zhang</a>
  - dropblock | 2018- <a href="https://arxiv.org/abs/1810.12890">Ghiasi, Dropblock</a>
  - improved learning rate | 2016- <a href="https://arxiv.org/abs/1608.03983">Ilya, SGDR</a>; 2017- <a href="https://arxiv.org/abs/1706.02677">Kaiming, Large Mini-batch Learning Rate</a>; <a href='https://arxiv.org/abs/1711.05101'>Ilya, AdamW</a>
  - repeatitive sampling | <a href="https://arxiv.org/abs/1901.09335"> Hoffer </a>
  - adversarial augmentation | 2020-<a href="https://arxiv.org/pdf/1912.11188.pdf">Zhang </a>  2020-<a href="https://arxiv.org/abs/2002.10876">Li</a>
